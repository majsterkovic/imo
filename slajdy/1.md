## Inteligentne metody optymalizacji

### Formułowanie zagadnienia optymalizacji

1. Jaki cel chcemy osiągnąć?
minimalizacja kosztów rozwizienia towarów do klientów
2. Jakie decyzje możemy podjąć?
przydział klientów do pojazdów, kolejność odwiedzania klientów
3. Jakie ograniczenia musimy uwzględnić?
pojemność pojazdów, czas pracy kierowców, okna czasowe klientów
4. Jak obliczyć wpływ naszych decyzji na cel?
analityczne, funkcja liniowa

### Model matematyczny - sformułowanie zagadnienia optymalizacji
minimalizuj / maksymalizuj z = f(x)
przy ograniczeniach
x ∈ S

minimalizuj z = f(x)
jest równoważne
maksymalizuj z' = -f(x)

### Rozwiązanie optymalne

Dla minimalizacji rozwiązanie x^opt € S, które spełnia f(x^opt) ≤ f(x) dla każdego x € S

Dla maksymalizacji rozwiązanie x^opt € S, które spełnia f(x^opt) ≥ f(x) dla każdego x € S

### Problem optymalizacji kombinatorycznej

Problem optymalizacji kombinatorycznej to zbior instancji definiowanych według pewnego schematu.

Instancja to konkretne zagadnienie optymalizacji - funkcja celu, przestrzeń decyzyjna, ograniczenia, kierunek optymalizacji

Instancja powstaje poprzez ustalenie pewnych parametrów problemu, np. liczby wierzchołków i odległości pomiędzy wierzchołkami w problemie komiwojażera

### Przykłady problemów optymalizacji kombinatorycznej

1. Problem przydziału (assignment)
Dany jest zbiór elementów i zadań. Każdy element może być przydzielony do każdego zdania. Ten przydział wiąże się z kosztem. Do każdego zadania można przydzielić tylko jeden element. Należy znaleźć taki przydział, który minimalizuje łączny koszt.
Np. przydział pracowników do zadań

2. Problem plecakowy (knapsack)
Dany jest zbiór elementów o danej wadze i wartości. Należy wybrać elementy do umieszczenia w plecaku, tak aby nie przekroczyć pojemności plecaka i maksymalizować łączną wartość elementów.
Np. złodziej wybierający przedmioty do kradzieży, inwestor wybierający akcje do zakupu

3. Problem minimalnego drzewa rozpinającego (minimum spanning tree)
Należy znaleźć drzewo rozpinające (zbiór krawędzi łączących wszystkie 
wierzchołki) w grafie ważonym o minimalnej łącznej wadze
Np. planowanie połączeń telekomunikacyjnych o minimalnym 
łącznym koszcie łączących wszystkie wierzchołki

4. Problem pokrycia zbioru (set covering problem)
Dany jest zbiór elementów i zestaw podzbiorów tych elementów. 
Każdy podzbiór ma pewien koszt. Należy pokryć wszystkie elementy 
wybierając podzbiory o minimalnym łącznym koszcie


5. Problem komiwojażera (traveling salesperson problem - TSP)
Dany jest zbiór wierzchołków (miast) i odległości pomiędzy każdą parą 
wierzchołków (pełen graf ważony). Należy znaleźć cykl Hamiltona 
(ścieżkę zamkniętą przechodzącą przez wszystkie wierzchołki) o 
minimalnej łącznej długości
Np. Częsty element praktycznych problemów transportowych

6. Problem marszrutyzacji pojazdów (vehicle routing problem - VRP)
Dany jest zbiór wierzchołków wraz z odległościami i parametry floty 
pojazdów (np. pojemność, okna czasowe), w tym wierzchołek bazowy. 
Należy znaleźć zbiór tras odwiedzających wszystkie wierzchołki, 
zaczynających i kończących się w bazie, spełniających wszystkie 
ograniczenia o minimalnej łącznej długości


7. Problem kolorowania grafu
Należy pokolorować (przydzielić etykiety) wierzchołkom niepełnego 
grafu, tak aby wierzchołki o tym samym kolorze nie były połączone 
krawędzią, używając jak najmniejszej liczby kolorów
Np. przydział częstotliwości do anten w sieciach komórkowych 
(krawędź oznacza ryzyko interferencji)

8. Grupowanie (clustering)
Należy podzielić zbiór elementów dla których znana jest pewna miar 
podobieństwa lub odległości minimalizując pewną miarę jakości 
grupowania, np. stosunek średniej odległości rozwiązań 
umieszczonych w jednej grupie do średniej odległości rozwiązań 
umieszczonych w różnych grupach

### Żródła trudności zagadnień optymalizacji
- duża liczba możliwych rozwiązań
np. problem problem plecakowy z n elementami ma 2^n możliwych rozwiązań
- ograniczenia
trudniejsze staje się przechodzenie między rozwiązaniami dopuszczalnymi
- złożoność obliczeniowa
problemy klasy P (wielomianowa) i NP (wykładnicza)
- niepewność (nierozważana)

### Klasyfikacja metod optymalizacji

1. Metody dokładne (dają gwarancję optymalnego rozwiązania)
pełny przegląd, metody podziału i ograniczeń, metody programowania matematycznego, programowanie dynamiczne
2. Metody przybliżone, heurystyki (mogą lub nie dawać gwarancję przybliżenia)
losowe błądenie, zachłanne heurystyki konstrukcyjne, metaheurystyki (inteligentne heurystyki), metody z ograniczonym czasem 

1a. Pełen przegląd (brute force)
- generowanie i ocena wszystkich możliwych rozwiązań
- trudne lub niemożliwe dla dużych instancji
- wymaga systematycznego przeglądu przestrzeni rozwiązań np. dla ciągów binarnych dekodowanie liczb całkoitych
- często organizowane w drzewo przestrzeni rozwiązań

1b. Metoda podziału i ograniczeń
- modyfikacja przeglądu drzewa
- przed rozwinięciem gałęzi obliczana jest wartość dolnego lub górnego ogranizenia optymalnej wartości funkcji celu w tej gałęzi (lower/upper bound)
- jeżeli ograniczenie jest gorsze od dotychczasowego najlepszego rozwiązania, gałąź jest obcinana
- jeżeli przerywamy przed rozwinięciem całego drzewa metoda staje się heurystyką

2a. Przeszukiwanie losowe (random search)
- generuj losowe rozwiązania i oceniaj je dopóki nie zostaną spełnione kryteria stopu

2b. Losowe błądzenie (random walk)
- wygeneruj losowe rozwiązanie, dopóki nie zostanie spełnione kryterium stopu modyfikuj je

2c. Heurystyki konstrukcyjne
- rozwiązanie jest konstruowane krokowo poprzez dokładanie kolejnych elementów

2d. Zachłanne heurystyki konskrukcyjne
- powstają jezeli kolejne elementy są wybierane tak, aby optymalizować zmianę wartości funkcji celu

2e. Heurysytka najbliższego sąsiada (dla TSP)
- wybierz wierzchołek startowy
- dodaj do cyklu najbliższy nieodwiedzony wierzchołek

2f. Metoda rozbudowy cyklu (dla TSP)
- wybierz wierzchołek startowy
- wybierz najbliższy nieodwiedzony wierzchołek i stwórz niepełny cykl
- wstaw w najlepsze miejsce w cyklu wierzchołek, który minimalizuje długość cyklu

2g. Heurystyka Clarke-Wrighta (dla VRP)
- połącz każdy wierzchołek z bazą
- wybierz i połącz w jedną dwie trasy, które dają najlepszą poprawę wartości funkcji celi i których połączenie jest dopuszczalne

### Optymalność algorytmów zachłannych

Algorytmy są optymalne dla problemów, które są ważonymi matroidami z nieujemnymi wagami, gdzie waga sumy elementów jest sumą ich wag (np. MST)

Matroid to para (E, I), gdzie E to zbiór elementów, a I to zbiór podzbiorów E spełniających warunki:
- zbiór pusty jest niezależny
- każdy podzbiór zbioru niezależnego jest niezależny
- wszystkie maksymalne niezależne zbiory mają taką samą moc

### Aproksymacja algorytmów zachłannych

- jeżeli funkcja celu jest submodularna i monotoniczna
- subomdlarna - dodawanie do nadzbioru nie może być bardziej opłacalne niż dodawanie do podzbioru
Przykłady: MaxCover problem, Hypervolume Subset Selection problem

Powyższe założenia mogą być spełnione w przybliżeniu i efektem może być generowanie rozwiązań znacznie lepszych niż losowe

### Randomizacja heurystyk zachlannych

Wadą heurystyk zachłannych jest ich determinizm lub niewielka możliwość randomizacji.
Np. wybór wierzchołka w heurystyce najbliższego sąsiada

### Greedy Randomized Adaptive Search Procedure (GRASP)
Utwórz niepelne rozwiązanie początkowe (np. zbiór pusty)
Dopóki nie utworzono pełnego rozwiązania:
- stwórz ograniczoną listę kandydatów (RCL - restricted candidate list) z pewną liczbą najlepszych elementów nie wchodzących jeszcze w skład rozwiązania x
- dodaj do x losowy element z RCL

Jak tworzyć RCL?
- Reguła ilościowa: wybieramy p% najlepszych elementów
- Regula wartościowa: zmiana funkcji celu w ramach określonego progu

### Inny sposób randomizacji
Utwórz rozwiązanie początkowe
Przejrzyj p% losowych elementów i dodaj najlepszy do rozwiązania

### Heurystyki zachłanne oparte na żalu (regret) heuristics

k-żal to suma różnicy między wartością funkcji celu dla najlepszego elementu a wartością funkcji celu dla k-1 kolejnymi opcjami wstawienia

Wybieramy element o największym żalu i wstawiamy go w najlepsze miejsce

### Beam search

Zamiast wstawiania jednego elementu możemy oceniać wstawianie kilku do przodu

Metoda ta łączy podejście heurystyczne z częściowym przeszukiwaniem drzewa rozwiązań

### Lokalne przeszukiwanie

Sąsiedztwo - topologia przestrzeni rozwiązań (dopuszczalnych)

- Sąsiedztwo oparte na odległości:
zbiór N(x) rozwiazań, które różnią się od x o nie więcej niż d
- Sąsiedztwo oparte na ruchach:
zbiór N(x) rozwiazań, które różnią się od x przez wykonanie jednej modyfikacji

### Pożądane cechy sąsiedztwa
- jeśli y ∈ N(x) to x ∈ N(y) (transformacja jest odwracalna)
- Ograniczenia na rozmiar: N(x) musi zawierać co najmniej jedno rozwiązanie różne od x, rozmiar powinien być dużo mniejszy niż liczba wszystkich rozwiązań
- wykonanie transformacji m powinno być prostsze niż skonstruowanie rozwiązania od zera uwzględniając przeliczanie funkcji celu i ograniczeń
- równouprawnienie (z każdego rozwiązania można przejść do każdego innego)

### Typowe operatory sąsiedztwa
- zamiana wartości jednego elementu
- przesunięcie elementu z jednego zbioru do drugiego
- wymiana dwóch elementów pomiędzy zbiorami
- przesuniecie elementu w sekwencji
- zamiana pozycji elementów w sekwencji
- odwórcenie kolejności części sekwencji

### Idea lokalnego przeszukiwania (local search)

- Tworzymy rozwiązanie początkowe (losowe lub za pomocą zrandomizowanej heurystyki)
- Poprawiamy rozwiązanie przechodząc do sąsiednich rozwiązań tak długo jak istnieją ruchy przynoszące poprawę

W wersji zachłannej (greedy) wybieramy pierwsze lepsze rozwiązanie z N(x), a w wersji stromej (steepest) wybieramy najlepsze rozwiązanie z N(x)

### Efektywność lokalnego przeszukiwania
W LP operacją krytyczną jest ocena sąsiednich rozwiązań. Żeby ocenić rozwiązanie wystarczy obliczyć deltę funkcji celu.

Można wykorzystywać oceny ruchów z poprzednich iteracji.

### Uporządkowana wg delty lista ruchów przynooszących poprawę

Zainicjuj LM - listę ruchów przynoszących poprawę uporządkowaną wg delty
Wygeneruj rozwiązanie początkowe x
dopóki nie znaleziono aplikowalnego ruchu z LM:
przejrzyj wszystkie nowe ruchy i dodaj do LM te, które przynoszą poprawę
wybierz najlepszy ruch z LM i zastosuj go do x
Jeśli ruch nie jest aplikowalny, usuń go z LM

### Globalna pamięć delt

Można zapamiętywać wszystkie obliczone delty w pamięci globalnej przy wielokrotnym uruchamianiu LP

### Ruchy kandydackie

Aby nie oceniać nieobiecujących ruchów, w sąsiedztwie ocenia się ruchy kandydackie. W wersji idealnej skracamy czas obliczeń bez utraty jakości rozwiązania, w paktyce jeśli pewne dobre ruchy nie są na liście kandydackich, następuje pogorszenie jakości rozwiązania.

### Wady lokalnego przeszukiwania

- kończą działane w optimum lokalnym
- jakość optimum zależy od rozwiązania początkowego

### Unikanie wad LP

- rozszerzenie definicji sąsiedztwa
- akceptowanie gorszych rozwiązań
- uruchamianie LP z różnych rozwiązań początkowych
- generowanie dobrych rozwiązań początkowych

### Multiple start local search

Wielokrotne uruchomienie LP z różnych rozwiązań początkowych. Zwracamy najlepsze rozwiązanie.

### Variable neighborhood local search

Rozpoczynamy od sąsiedztw, które są najmniej złożone, a następnie przechodzimy do coraz bardziej złożonych.

### Iterated local search

Wygeneruj rozwiązanie początkowe
Lokalne przeszukiwanie
Dopóki nie spełniono kryterium stopu:
- perturbacja
- lokalne przeszukiwanie

### Perturbacja
Perturbacja to zrandomizowana modyfikacja, która wykracza poza sąsiedztwo wykorzystywane w LP. Jest nieosiągalna w jednym ruchu.

Zbyt mała perturbacja może prowadzić do powrotu do optimum lokalnego, zbyt duża do utraty jakości rozwiązania.

### Large-scale neighborhood search - wielkoskalowe przeszukiwanie sąsiedztwa

Ruch polega na złożeniu dwóch metod:
- destroy - usunięcie części rozwiązania
- repair - naprawienie rozwiązania ukierunkowane funkcją celu

### Hiperheurystyki

- w dowolnej iteracyjnej heurystyce mamy możliwość stosowania różnych operatorów
- ich prawdopodobieństwo jest automatycznie modyfikowane na podstawie efekttów stosowania

### Kryteria stopu
- czas iteracji / obliczeń
- dynamiczne - brake poprawy w zadanej liczbie iteracji / zadanym czasie

### Simulated annealing - symulowane wyżarzanie
Fizycznie wyżarzanie to proces stopniowego schładzania stopu do temperatury pokojowej.

Lepsze rozwiązania są zawsze akceptowane, gorsze z pewnym prawdopodobieństwem.
Im mniejsze pogorszenie, tym większe prawdopodobieństwo akceptacji.
im wyższa temperatura, tym większe prawdopodobieństwo akceptacji gorszego rozwiązania.

Przy wystarczająco dużej temperaturze początkowej i wystarczająco powolnym obniżaniu temperatury prawdopodobieństwo uzyskania optimum globalnego dąży do 1
Wynik raczej teoretyczny, w praktyce wymagana liczba ruchów podobna do pełnego przeglądu

### Great Deluge - wielka powódź
Wielka powódź to uogólnienie symulowanego wyżarzania, w którym temperatura jest zastąpiona przez poziom wody, który stopniowo rośnie.

### Tabu search - przeszukiwanie z listą tabu
Puktem wyjścia jest LP w werjsi stromej.
Aby uniknąć powrotu do poprzednich rozwiązań, stosujemy listę tabu, która zawiera ruchy, które nie mogą być ponownie wykonane.

Krótka lista powoduje bardziej agresywny algorytm z ryzykiem wpadnięcia w cykl. Długa lista daje ryzyko pomijania dobrych ruchów.


### Reactive tabu search
W RTS długość listy tabu jest dynamicznie modyfikowana w zależności od efektów działania algorytmu.
Lista jest zmniejszana co L iteracji. Jeśli algorytm wpadnie w cykl, lista jest zwiększana.

### Kryteria aspiracji

Ruchy tabu mogą być akceptowane, jeśli spełnione są pewne warunki, np. ruch prowadzi do najlepszego rozwiązania.

### Pamięć długoterminowa - long-term memory

Zastosowania:
- restarty - dywersyfikacja / intensyfikacja (rozwiązania startowe odległe lub podobne do najlepszych dotychczasowych)